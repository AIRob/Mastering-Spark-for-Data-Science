{"paragraphs":[{"title":"Load Dependencies","text":"%dep\nz.reset\n//\n// This is a reusable zeppelin notebook for building the geoGCAM Sentiment SpatioTemporal data feed from GDELT's GKG files (available from gdeltproject.org)\n// It was created as a proof of concept for constructing the geoGCAM dataset. Tested on Spark 1.6. Your mileage may vary!\n\n// Author: Andrew Morgan\n// \n// It is described in the book \"Mastering Spark for Data Science\" by Packt Publising.\n//\n// ************** License\n// *\n// * Licensed under the Apache License, Version 2.0 (the \"License\");\n// * you may not use this file except in compliance with the License.\n// * You may obtain a copy of the License at\n// *\n// *      http://www.apache.org/licenses/LICENSE-2.0\n// *\n// * Unless required by applicable law or agreed to in writing, software\n// * distributed under the License is distributed on an \"AS IS\" BASIS,\n// * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// * See the License for the specific language governing permissions and\n// * limitations under the License.\n// */\n// ****************\n//\n// DEPENDENCIES\n//\n// this dependency needed for versions of spark 1.x, for spark 2.x it's not needed\nz.load(\"com.databricks:spark-csv_2.10:1.5.0\")   // can be removed if using spark 2.0+\n// other libraries\nz.load(\"com.github.davidmoten:geo:0.7.1\")       // there are other libraries, but I like this one. Used it for geohashing\nz.load(\"org.apache.hadoop:hadoop-hdfs:2.5.2\")   // needed to control HDFS via Spark\n","dateUpdated":"Apr 27, 2017 10:58:18 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315648_304459408","id":"20170426-142835_861072983","result":{"code":"SUCCESS","type":"TEXT","msg":"res1: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@7ae94740\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:58:18 PM","dateFinished":"Apr 27, 2017 10:58:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2947","focus":true},{"text":" Introduction to geoGCAM:\n\n This notebook is designed as a POC to develop a feed of large scale spatio-temporal sentiment data from the GDELT's GKG files.\n It also demonstrates that a zeppelin notebook can be considered a unit of work, allowing mixed work loads (shell / spark) to be deployed as one job.\n\n Functionality and usage:\n\n This notebook will use the Bash context (tested on Centos) to query for any un-processed gkg files in our landing directory (presently for english language)\n and then calculate the to-do list of files to process to keep up to date. Then it will process them sequentially. \n That's done for a particular reason. There is a huge number of files in my backlog to process, and I'm incrementally doing it during periods of low activity on my cluster.\n Setting up the job this way allows me to tune the number of files to include so that it fits within the nightly processing windows I have available. \n \n Also, if there's a failure this script is restartable, or if you need to turn it off for a while, it will pick up and restart where it left off.\n \n While this is not a pattern in the book (it was created way before our NIFI ingester was running) it works on my small dev machine and crunched through thousands of files in\n the backlog I wanted to pick up. I also ran it incrementally in small batches, as I needed to ensure enough resources were free for other more important processes.\n \n Why I like it is that it picks up new files to run each execution, as well as included some of the back log files, so that slowly over time I have processed\n the entire backlog.\n \n CRON: \n Set up a cron to run this notebook after adjusting the inputs and outputs to match your hdfs directories. Tips found in the code.\n \n What it does:\n The purpose of the script is to explode the GKG GCAM (sentiment data) and the location data (US and World cities) for all news stories in scope in the 15min\n window - and then run the following pre-processing:\n \n Build a cartesian join between them. \n \n Aggregate across the entire 15minute window of news stories, summing the sentiment scores by geohashes to \"spatially pivot\" the data.\n By doing this step we obtain a map of total News Sentiment by Location - for thousands of sentiments at city level globally.\n \n Improvements that should be considered if you productionise this process:\n    Rebuild the pipeline to hook it up to the Universal Ingester.\n    Have the output load directly to geomesa - via a process documented in Chapter 5.\n    Have this process also pick up the translingual gkg files, to construct a truly global view of spatial news sentiment\n","dateUpdated":"Apr 27, 2017 10:55:06 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":false,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213689511_481331699","id":"20170426-143449_1814127467","dateCreated":"Apr 26, 2017 2:34:49 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2948","title":"Notes","focus":true},{"title":"Set the Cron for this notebook","text":"// my version of zeppelin allows for a Cron job to be defined. \n\n// the cron settings for the notebook are:\n// 0 0 0/3 * * ?\n// this runs the notebook every three hours. The settings are tuned for my dev machine to \n// finish processing in about 2 hours 45 minutes after launching. \n// You may want to adjust for your setup the number of files to process.","dateUpdated":"Apr 27, 2017 10:58:21 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1492242301","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:58:21 PM","dateFinished":"Apr 27, 2017 10:58:56 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2949","focus":true},{"title":"How to Generate Filenames to process as a restartable to do list","text":"%sh\n\n# apologies for the over use of shell here, but this is an EDA prototype! \n# you will need to set your directories below carefully to pick your data, and to write outputs:\n\n#sed 's/^.*\\///g\ndate\n\n# try: whereis hdfs to configure HADOOP_BIN\n\nHADOOP_BIN=/usr/bin\nGDELT_DIR=/user/feeds/gdelt/gkg2eng\nDATASTORE_DIR=/user/feeds/gdelt/datastore\nDATASTORE_OUTOUTS_DIR=/user/feeds/gdelt/datastore/GcamGeo\n\n# Create the datastore dir if not exists\n$HADOOP_BIN/hdfs dfs -mkdir -p $DATASTORE_DIR\n\n# Run the scripts separately to regenerate the current list of files in gkg2eng \n\necho \"filename\" >  gkg2eng_allfilenames_00.txt\n\n$HADOOP_BIN/hdfs dfs -ls -R $GDELT_DIR | awk '/csv/ {print $8}' | sed 's/^.*\\(\\/2016.\\/\\)/\\1/g'  > temp_gkg2eng_allfilenames_01.txt\n\n# before we write the \"todo\" list to hdfs, we also query the completed files, and remove ones done so far. \n$HADOOP_BIN/hdfs dfs -ls $DATASTORE_OUTOUTS_DIR/GCAM_2* | sed 's/^.*\\///; s/GCAM_\\(....\\)/\\/\\1\\/\\1/; s/.gkg/00\\.gkg/' > temp_gkg2eng_allfilenames_02.txt\n\n# Now we can merge the todo files, with the done files, and produce yet unprocessed files. Then we load them up for processing later.\necho \"-----qc\"\n\necho \"# temp_gkg2eng_allfilenames_01.txt: the master list of files to process\"\nhead -5 temp_gkg2eng_allfilenames_01.txt\n\necho \"# temp_gkg2eng_allfilenames_02.txt: the files we already completed\"\ntail -5 temp_gkg2eng_allfilenames_02.txt\n\ncat  temp_gkg2eng_allfilenames_01.txt  temp_gkg2eng_allfilenames_02.txt | sort | uniq -u > gkg2eng_allfilenames_01.txt\n\necho \"#  gkg2eng_allfilenames_01.txt: the now uniq list of unprocessed files still to go\"\nhead -5  gkg2eng_allfilenames_01.txt\necho \"-----qc\"\nrm  temp_gkg2eng_allfilenames_*.txt\n\n$HADOOP_BIN/hdfs dfs -rm $DATASTORE_DIR/gkg2eng_allfilenames_01.txt\n$HADOOP_BIN/hdfs dfs -rm $DATASTORE_DIR/gkg2eng_allfilenames_00.txt\n$HADOOP_BIN/hdfs dfs -put ./gkg2eng_allfilenames_00.txt $DATASTORE_DIR/gkg2eng_allfilenames_00.txt\n$HADOOP_BIN/hdfs dfs -put ./gkg2eng_allfilenames_01.txt $DATASTORE_DIR/gkg2eng_allfilenames_01.txt\n\n\n# this file can now be read into spark to drive extracting the timeseries\n\n$HADOOP_BIN/hdfs dfs -cat  $DATASTORE_DIR/gkg2eng_allfilenames_*.txt | sort -r| head -30\ndate\n","dateUpdated":"Apr 27, 2017 10:58:23 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"HADOOP_HOME":""},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1770330299","result":{"code":"SUCCESS","type":"TEXT","msg":"Thu 27 Apr 22:58:23 BST 2017\n-----qc\n# temp_gkg2eng_allfilenames_01.txt: the master list of files to process\n/user/feeds/gdelt/gkg2eng/2015/20150218230000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2015/20150218231500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2015/20150218233000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2015/20150218234500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2015/20150219000000.gkg.csv\n# temp_gkg2eng_allfilenames_02.txt: the files we already completed\n/2016/20161219213000.gkg.csv\n/2016/20161219214500.gkg.csv\n/2016/20161219220000.gkg.csv\n/2016/20161219221500.gkg.csv\n/2016/20161219223000.gkg.csv\n#  gkg2eng_allfilenames_01.txt: the now uniq list of unprocessed files still to go\n/2000/20000000000000.gkg.csv\n/2015/20150218230000.gkg.csv\n/2015/20150218231500.gkg.csv\n/2015/20150218233000.gkg.csv\n/2015/20150218234500.gkg.csv\n-----qc\n17/04/27 22:58:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://gzet.bytesumo.com:8020/user/feeds/gdelt/datastore/gkg2eng_allfilenames_01.txt' to trash at: hdfs://gzet.bytesumo.com:8020/user/zeppelin/.Trash/Current\n17/04/27 22:58:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://gzet.bytesumo.com:8020/user/feeds/gdelt/datastore/gkg2eng_allfilenames_00.txt' to trash at: hdfs://gzet.bytesumo.com:8020/user/zeppelin/.Trash/Current\n/user/feeds/gdelt/gkg2eng/2017/20170222120000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222114500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222113000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222111500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222110000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222104500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222103000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222101500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222100000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222094500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222093000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222091500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222090000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222084500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222083000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222081500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222080000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222074500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222073000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222071500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222070000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222064500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222063000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222061500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222060000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222054500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222053000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222051500.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222050000.gkg.csv\n/user/feeds/gdelt/gkg2eng/2017/20170222044500.gkg.csv\nThu 27 Apr 22:58:51 BST 2017\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:58:23 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2950","dateFinished":"Apr 27, 2017 10:58:51 PM","focus":true},{"title":"Import Libraries","text":"import sys.process._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,  DoubleType}\nimport org.apache.spark.sql.SaveMode\nimport sqlContext.implicits._\nimport java.util.Calendar\n\nimport com.github.davidmoten.geo._\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs._\n \ndef merge(srcPath: String, dstPath: String): Unit =  {\n  val hadoopConfig = new Configuration()\n  val hdfs = FileSystem.get(hadoopConfig)\n  FileUtil.copyMerge(hdfs, new Path(srcPath), hdfs, new Path(dstPath), true, hadoopConfig, null) \n  // the \"true\" setting deletes the source files once they are merged into the new output\n}\n\nvar currtime = Calendar.getInstance().getTime()","dateUpdated":"Apr 27, 2017 10:58:32 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1764486341","result":{"code":"SUCCESS","type":"TEXT","msg":"import sys.process._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType}\nimport org.apache.spark.sql.SaveMode\nimport sqlContext.implicits._\nimport java.util.Calendar\nimport com.github.davidmoten.geo._\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs._\nmerge: (srcPath: String, dstPath: String)Unit\ncurrtime: java.util.Date = Thu Apr 27 22:58:59 BST 2017\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:58:32 PM","dateFinished":"Apr 27, 2017 10:58:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2951","focus":true},{"title":"Set out the GeoGCAM pipeline processes","text":"// This section defines the extraction process from raw GKG\n// The output is the geoGCAM dataset described in Chapter 4 of the book.\n\nvar currtime = Calendar.getInstance().getTime()\n\n\n// we define a geoGCAM extraction function:\n\ndef ExtractTimeSeries ( fileName: String = \"/user/feeds/gdelt/gkg2eng/2016/20161010*\") : org.apache.spark.sql.DataFrame  = {\n\n    // the filename points to the gkgfiles(s) to extract the timeseries from\n    // the GcamPrefix is the catagory of timeseries to extract. c18 = the GDELT core measures\n    \n    val textFileName = fileName.toString\n    \n    val GkgCoreSchema = StructType(Array(\n        StructField(\"GkgRecordId\"           , StringType, true), //$1       \n        StructField(\"V21Date\"               , StringType, true), //$2       \n        StructField(\"V2SrcCollectionId\"     , StringType, true), //$3       \n        StructField(\"V2SrcCmnName\"          , StringType, true), //$4\n        StructField(\"V2DocId\"               , StringType, true), //$5\n        StructField(\"V1Counts\"              , StringType, true), //block $6\n        StructField(\"V21Counts\"             , StringType, true), //block $7\n        StructField(\"V1Themes\"              , StringType, true), //block $8\n        StructField(\"V2Themes\"              , StringType, true), //block $9\n        StructField(\"V1Locations\"           , StringType, true), //block $10\n        StructField(\"V2Locations\"           , StringType, true), //block $11\n        StructField(\"V1Persons\"             , StringType, true), //block $12\n        StructField(\"V2Persons\"             , StringType, true), //block $13    \n        StructField(\"V1Orgs\"                , StringType, true), //block $14\n        StructField(\"V2Orgs\"                , StringType, true), //block $15\n        StructField(\"V15Tone\"               , StringType, true), //$16\n        StructField(\"V21Dates\"              , StringType, true), //$17\n        StructField(\"V2GCAM\"                , StringType, true), //$18\n        StructField(\"V21ShareImg\"           , StringType, true), //$19\n        StructField(\"V21RelImg\"             , StringType, true), //$20\n        StructField(\"V21SocImage\"           , StringType, true), //$21\n        StructField(\"V21SocVideo\"           , StringType, true), //$22\n        StructField(\"V21Quotations\"         , StringType, true), //$23\n        StructField(\"V21AllNames\"           , StringType, true), //$24\n        StructField(\"V21Amounts\"            , StringType, true), //$25\n        StructField(\"V21TransInfo\"          , StringType, true), //$26\n        StructField(\"V2ExtrasXML\"           , StringType, true)  //$27\n        ))\n    \n    \n    val GkgFileRaw = sqlContext.read\n                               .format(\"com.databricks.spark.csv\")\n                               .option(\"header\", \"false\")                               // Use first line of all files as header\n                               .schema(GkgCoreSchema)\n                               .option(\"delimiter\", \"\\t\")                               // tab delimited input\n                               .load(textFileName)\n    \n    GkgFileRaw.registerTempTable(\"GkgFile\")\n    //GkgFileRaw.show(10) \n    \n    val GcamRaw = GkgFileRaw.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"V2GCAM\", \"V1Locations\")\n        GcamRaw.cache()\n        GcamRaw.registerTempTable(\"GcamRaw\")\n        \n    \n    // here we wrap up the imported geohash functions - which allows us to register it as a UDF. \n    \n    def vgeoWrap (lat: Double, long: Double, len: Int): String = {\n        var ret = GeoHash.encodeHash(lat, long, len)\n        // select the length of the geohash, less than 12..\n        // it pulls in the library dependency from \n        return(ret)\n    }\n    \n    sqlContext.udf.register(\"vGeoHash\", vgeoWrap(_:Double,_:Double,_:Int))\n    \n    val ExtractGcam = sqlContext.sql(\"\"\"\n        select\n            GkgRecordID \n        ,   V21Date\n        ,   split(V2GCAM, \",\")                              as Array        -- output is an array\n        ,   explode(split(V1Locations, \";\"))                as LocArray     -- I pick up all the locations and later filter.\n        ,   regexp_replace(V15Tone, \",.*$\", \"\") as V15Tone                  -- I trunc off the other scores\n        from GcamRaw \n        where \n            length(V2GCAM) >1 \n        and length(V1Locations) >1\n    \"\"\")\n    \n    val explodeGcamDF = ExtractGcam.explode(\"Array\", \"GcamRow\"){c: Seq[String] => c }\n\n    val GcamRows = explodeGcamDF.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"GcamRow\", \"LocArray\") //,\"PersonName\")\n    GcamRows.registerTempTable(\"GcamRows\")\n    \n    val TimeSeries = sqlContext.sql(\"\"\"\n    select \n      d.V21Date\n    --, substr(d.V21Date,9, 2) as HH                        -- disabled: but the hour of the day can be calc'd here if needed\n    , d.LocCountryCode\n    , d.Lat\n    , d.Long\n    , vGeoHash(d.Lat, d.Long, 12) as GeoHash\n    , 'E' as NewsLang                                       -- this needs adjusting if you include translingual data later\n    , regexp_replace(Series, \"\\\\.\", \"_\") as Series          -- using periods or dots isn't great for lots of reasons downstream\n    , coalesce(sum(d.Value),0)           as SumValue        -- in SQL \"coalesce\" means \"replaces nulls with\" ;-)\n    , count(distinct  GkgRecordID )      as ArticleCount    -- useful for normalisation potentially\n    , Avg(V15Tone)                       as AvgTone         -- a simple average value of Tone is taken. More complex strategies possible.           \n    from\n    (   select\n        GkgRecordID\n        , V21Date\n        , ts_array[0] as Series                             -- note we can select fields from arrays by index values\n        , ts_array[1] as Value\n        , loc_array[0] as LocType\n        , loc_array[2] as LocCountryCode\n        , loc_array[4] as Lat\n        , loc_array[5] as Long\n        , V15Tone\n        from\n           (select\n             GkgRecordID\n           , V21Date\n           , split(GcamRow,   \":\") as ts_array\n           , split(LocArray, \"#\") as loc_array\n           , V15Tone\n           from GcamRows\n           where length(GcamRow)>1\n           ) x\n        where\n        (loc_array[0] = 3 or  loc_array[0] = 4)                     -- this limits the locations to just US and world Cities.\n\n       --  ## I am not using filters / constraints over the metrics to collect in GeoGCAM, but you can do so below ## \n       --   and\n       --   (       x.ts_array[0] like 'c18%' -- c18 denotes the gdelt specific timeseries, which are word dictionary counts.   c18.85=state of emergency\n       --      or   x.ts_array[0] = 'wc')\n    ) d\n    group by \n      d.V21Date\n    , d.LocCountryCode\n    , d.Lat\n    , d.Long\n    , vGeoHash(d.Lat, d.Long, 12)\n    , d.Series\n    order by \n      d.V21Date\n    , vGeoHash(d.Lat, d.Long, 12)\n    , d.Series\n    \"\"\")\n    \n    TimeSeries.repartition(10)\n       return TimeSeries\n\n    \n} // end of the function definition\n\n\nvar currtime = Calendar.getInstance().getTime()","dateUpdated":"Apr 27, 2017 10:59:01 PM","config":{"lineNumbers":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1294960609","result":{"code":"SUCCESS","type":"TEXT","msg":"currtime: java.util.Date = Thu Apr 27 22:59:01 BST 2017\nExtractTimeSeries: (fileName: String)org.apache.spark.sql.DataFrame\ncurrtime: java.util.Date = Thu Apr 27 22:59:03 BST 2017\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:59:01 PM","dateFinished":"Apr 27, 2017 10:59:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2952","focus":true},{"title":"Load Filenames list to process sequentially","text":"// Here I loop through the files to process. \n// I will write outputs to parquet as I go. \n\nvar currtime = Calendar.getInstance().getTime()\n\nval fileNamePath = \"/user/feeds/gdelt/datastore/gkg2eng_allfilenames_*.txt\"\nval files = sqlContext.read\n                      .format(\"com.databricks.spark.csv\")\n                      .option(\"header\", \"true\")                               // Use first line  as header found in 00_txt\n                      .option(\"delimiter\", \"\\t\")\n                      .load(fileNamePath)\nfiles.registerTempTable(\"files\")\n\n// files already processed are to be excluded:\nval filesProcessedPath = \"/user/feeds/gdelt/datastore/filesProcessed.csv\"\nval filesProcessed = sqlContext.read.parquet(filesProcessedPath)\nfilesProcessed.registerTempTable(\"filesProcessed\")\n\n//files.show(5, false)\n//This should return the filenames as an array of strings\n\n//\n// BELOW YOU CAN TUNE THE NUMBER OF FILES TO INCLUDE IN THE BACKLOG - use the limit clause in the sql to do it\n//\n\n\nval filearray = sqlContext.sql(\"\"\" \nselect \n  fileglob as filename \n-- , regexp_replace(regexp_replace(fileglob, \"^.*\\\\/\",\"\"), \"\\\\*.gkg.csv\", \"\") \nfrom (\n    Select \n        concat(substr(filename,1,18), \"*.gkg.csv\")  as fileglob\n        , filename\n        from files \n    ) v\nwhere  regexp_replace(regexp_replace(fileglob, \"^.*\\\\/\",\"\"), \"\\\\*.gkg.csv\", \"\") > \"201502191615\" \ngroup by fileglob\norder by fileglob DESC -- ASC\nlimit 75 --- set the files to process here. about 28 files takes 1 hour to process on a 12 core machine. 168 is about 6 hours.\n\"\"\")\n\nfilearray.show(100, false )\n\nvar currtime = Calendar.getInstance().getTime()","dateUpdated":"Apr 27, 2017 10:59:09 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_468923166","result":{"code":"SUCCESS","type":"TEXT","msg":"currtime: java.util.Date = Thu Apr 27 22:59:09 BST 2017\nfileNamePath: String = /user/feeds/gdelt/datastore/gkg2eng_allfilenames_*.txt\nfiles: org.apache.spark.sql.DataFrame = [filename: string]\nfilesProcessedPath: String = /user/feeds/gdelt/datastore/filesProcessed.csv\nfilesProcessed: org.apache.spark.sql.DataFrame = [filename: string]\nfilearray: org.apache.spark.sql.DataFrame = [filename: string]\n+---------------------------+\n|filename                   |\n+---------------------------+\n|/2016/201612192230*.gkg.csv|\n|/2016/201612192215*.gkg.csv|\n|/2016/201612192200*.gkg.csv|\n|/2016/201612192145*.gkg.csv|\n|/2016/201612192130*.gkg.csv|\n|/2016/201612192115*.gkg.csv|\n|/2016/201612192100*.gkg.csv|\n|/2016/201612192045*.gkg.csv|\n|/2016/201612192030*.gkg.csv|\n|/2016/201612192015*.gkg.csv|\n|/2016/201612192000*.gkg.csv|\n|/2016/201612191945*.gkg.csv|\n|/2016/201612191930*.gkg.csv|\n|/2016/201612191915*.gkg.csv|\n|/2016/201612191900*.gkg.csv|\n|/2016/201612191845*.gkg.csv|\n|/2016/201612191830*.gkg.csv|\n|/2016/201612191815*.gkg.csv|\n|/2016/201612191800*.gkg.csv|\n|/2016/201612191745*.gkg.csv|\n|/2016/201612191730*.gkg.csv|\n|/2016/201612191715*.gkg.csv|\n|/2016/201612191700*.gkg.csv|\n|/2016/201612191645*.gkg.csv|\n|/2016/201612191630*.gkg.csv|\n|/2016/201612191615*.gkg.csv|\n|/2016/201612191600*.gkg.csv|\n|/2016/201612191545*.gkg.csv|\n|/2016/201612191530*.gkg.csv|\n|/2016/201612191515*.gkg.csv|\n|/2016/201612191500*.gkg.csv|\n|/2016/201612191445*.gkg.csv|\n|/2016/201612191430*.gkg.csv|\n|/2016/201612191415*.gkg.csv|\n|/2016/201612191400*.gkg.csv|\n|/2016/201612191345*.gkg.csv|\n|/2016/201612191330*.gkg.csv|\n|/2016/201612191315*.gkg.csv|\n|/2016/201612191300*.gkg.csv|\n|/2016/201612191245*.gkg.csv|\n|/2016/201612191230*.gkg.csv|\n|/2016/201612191215*.gkg.csv|\n|/2016/201612191200*.gkg.csv|\n|/2016/201612191145*.gkg.csv|\n|/2016/201612191130*.gkg.csv|\n|/2016/201612191115*.gkg.csv|\n|/2016/201612191100*.gkg.csv|\n|/2016/201612191045*.gkg.csv|\n|/2016/201612191030*.gkg.csv|\n|/2016/201612191015*.gkg.csv|\n|/2016/201612191000*.gkg.csv|\n|/2016/201612190945*.gkg.csv|\n|/2016/201612190930*.gkg.csv|\n|/2016/201612190915*.gkg.csv|\n|/2016/201612190900*.gkg.csv|\n|/2016/201612190845*.gkg.csv|\n|/2016/201612190830*.gkg.csv|\n|/2016/201612190815*.gkg.csv|\n|/2016/201612190800*.gkg.csv|\n|/2016/201612190745*.gkg.csv|\n|/2016/201612190730*.gkg.csv|\n|/2016/201612190715*.gkg.csv|\n|/2016/201612190700*.gkg.csv|\n|/2016/201612190645*.gkg.csv|\n|/2016/201612190630*.gkg.csv|\n|/2016/201612190615*.gkg.csv|\n|/2016/201612190600*.gkg.csv|\n|/2016/201612190545*.gkg.csv|\n|/2016/201612190530*.gkg.csv|\n|/2016/201612190515*.gkg.csv|\n|/2016/201612190500*.gkg.csv|\n|/2016/201612190445*.gkg.csv|\n|/2016/201612190430*.gkg.csv|\n|/2016/201612190415*.gkg.csv|\n|/2016/201612190400*.gkg.csv|\n+---------------------------+\n\ncurrtime: java.util.Date = Thu Apr 27 22:59:27 BST 2017\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:59:09 PM","dateFinished":"Apr 27, 2017 10:59:27 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2953","focus":true},{"title":"Test Pipeline Function working.","text":"// test the file extractor is working:\n// val testing = ExtractTimeSeries(\"/user/feeds/gdelt/gkg2eng/2015/20150218231500.gkg.csv\") \n//testing.show(5, false)\n \n// test my regex worked:\n//\"/user/feeds/gdelt/gkg2eng/2015/2015021823*.gkg.csv\".replaceAll(\"^.*\\\\/\", \"\").replaceAll(\"\\\\*\", \"\")\n ","dateUpdated":"Apr 27, 2017 10:45:15 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1525850139","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 26, 2017 2:28:35 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2954"},{"title":"Knife and Fork: process the backlog of data here and write out","text":"// this solution below also demonstrates an interesting approach to consolidating outputs using HDFS via merges\n\nvar currtime = Calendar.getInstance().getTime()\n\nval filesToLoopThrough = filearray.rdd.map(f => f(0).asInstanceOf[String]).collect()    // turn the column data into an array of strings\n\nval outputfile = \"/user/feeds/gdelt/datastore/GcamGeo\"  // set out the file filepath by adding prefix to filename\nval sourcedir = \"/user/feeds/gdelt/gkg2eng\"\n\n// below we run the data pipeline function to generate our output, then write it to files, then use HDFS to merge them into our proper output datastore.\n// note - this would be a good place to load the data directly into GeoMesa or other indexed datastore too.\n// in my case, I required text files for various reasons. \n\nfor ( fname <- filesToLoopThrough ){\n     \n     var filetoprocess = sourcedir + fname\n     var newData = ExtractTimeSeries(filetoprocess)\n     \n     var outputFileName = outputfile + \"/GCAM_delta\" + fname.replaceAll(\"^.*\\\\/\", \"\").replaceAll(\"\\\\*\", \"\")\n     \n     var mergedFileName = outputfile + \"/GCAM_\" + fname.replaceAll(\"^.*\\\\/\", \"\").replaceAll(\"\\\\*\", \"\")\n     var mergeFindGlob  = outputFileName\n\n        newData.write\n            .format(\"com.databricks.spark.csv\")\n            .option(\"header\", \"false\")\n            .mode(\"overwrite\")\n            .save(outputFileName)\n        merge(mergeFindGlob, mergedFileName )\n        newData.unpersist()\n} \n// end of for loop\nvar currtime = Calendar.getInstance().getTime()","dateUpdated":"Apr 27, 2017 10:59:12 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315649_304074659","id":"20170426-142835_1812542020","result":{"code":"ERROR","type":"TEXT","msg":"currtime: java.util.Date = Wed Feb 08 17:18:25 GMT 2017\nfilesToLoopThrough: Array[String] = Array(/2016/201612192230*.gkg.csv, /2016/201612192215*.gkg.csv, /2016/201612192200*.gkg.csv, /2016/201612192145*.gkg.csv, /2016/201612192130*.gkg.csv, /2016/201612192115*.gkg.csv, /2016/201612192100*.gkg.csv, /2016/201612192045*.gkg.csv, /2016/201612192030*.gkg.csv, /2016/201612192015*.gkg.csv, /2016/201612192000*.gkg.csv, /2016/201612191945*.gkg.csv, /2016/201612191930*.gkg.csv, /2016/201612191915*.gkg.csv, /2016/201612191900*.gkg.csv, /2016/201612191845*.gkg.csv, /2016/201612191830*.gkg.csv, /2016/201612191815*.gkg.csv, /2016/201612191800*.gkg.csv, /2016/201612191745*.gkg.csv, /2016/201612191730*.gkg.csv, /2016/201612191715*.gkg.csv, /2016/201612191700*.gkg.csv, /2016/201612191645*.gkg.csv, /2016/201612191630*.gkg.csv, /2016/201612191615*.gkg.csv, /...outputfile: String = /user/feeds/gdelt/datastore/GcamGeo\nsourcedir: String = /user/feeds/gdelt/gkg2eng\njava.io.IOException: Target /user/feeds/gdelt/datastore/GcamGeo/GCAM_201612192230.gkg.csv already exists\n\tat org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:504)\n\tat org.apache.hadoop.fs.FileUtil.copyMerge(FileUtil.java:386)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.merge(<console>:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:97)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:80)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:110)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:112)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:116)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:118)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:120)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:122)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:124)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:126)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:128)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:130)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:132)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:134)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:136)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:138)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:140)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:142)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:144)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:146)\n\tat $iwC$$iwC$$iwC.<init>(<console>:148)\n\tat $iwC$$iwC.<init>(<console>:150)\n\tat $iwC.<init>(<console>:152)\n\tat <init>(<console>:154)\n\tat .<init>(<console>:158)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:673)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:666)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:295)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","dateStarted":"Apr 27, 2017 10:59:12 PM","dateFinished":"Apr 27, 2017 10:45:22 PM","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:2955","focus":true},{"text":"val currtime = Calendar.getInstance().getTime()\n\n// this will print the end time of the notebook run.","dateUpdated":"Apr 27, 2017 10:45:16 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315650_305228906","id":"20170426-142835_1606489027","result":{"code":"SUCCESS","type":"TEXT","msg":"currtime: java.util.Date = Wed Feb 08 17:20:54 GMT 2017\n"},"dateCreated":"Apr 26, 2017 2:28:35 PM","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2956"},{"dateUpdated":"Apr 27, 2017 10:45:16 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1493213315650_305228906","id":"20170426-142835_276595288","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 26, 2017 2:28:35 PM","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2957"}],"name":"Chapter 4.d GeoGCAM - Build Pipeline POC","id":"2CF4Y94T6","angularObjects":{"2BFGJGJMM":[],"2BFW8365E":[],"2BFANN9UW":[],"2BGYV6UXX":[],"2BFFW1KRP":[],"2BGE258H2":[],"2BE7JTP1M":[],"2BFHKKS5A":[],"2BH88Q541":[],"2BH9WQF49":[],"2BHFNSYHC":[],"2BF1MU2J1":[],"2BHEBWDK2":[]},"config":{"looknfeel":"default"},"info":{}}