{
  "paragraphs": [
    {
      "title": "Load Dependencies",
      "text": "%dep\nz.reset\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")\nz.load(\"com.github.davidmoten:geo:0.7.1\")",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:03 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183866_582044287",
      "id": "20160528-174757_359383186",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@2fd391b0\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:03 PM",
      "dateFinished": "Jun 3, 2016 5:20:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Import Libraries",
      "text": "import sys.process._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\nimport org.apache.spark.sql.SaveMode\nimport sqlContext.implicits._\n\nimport com.github.davidmoten.geo._\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183866_582044287",
      "id": "20160528-174757_1188387011",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import sys.process._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\nimport org.apache.spark.sql.SaveMode\nimport sqlContext.implicits._\nimport com.github.davidmoten.geo._\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:10 PM",
      "dateFinished": "Jun 3, 2016 5:20:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "FYI: How to Generate Filenames to process",
      "text": "%sh\n\n#sed \u0027s/^.*\\///g\n\nHADOOP_BIN\u003d/Users/uktpmhallett/Installs/hadoop/hadoop-2.6.2/bin\nGDELT_DIR\u003d/users/uktpmhallett/feeds/gdelt/gkg2eng\nDATASTORE_DIR\u003d/users/uktpmhallett/feeds/gdelt/datastore\n\n# Create the datastore dir if not exists\n$HADOOP_BIN/hdfs dfs -mkdir -p $DATASTORE_DIR\n\n# Run the scripts separately to regenerate the current list of files in gkg2eng \n\n$HADOOP_BIN/hdfs dfs -ls -R $GDELT_DIR | awk \u0027{print $8}\u0027 | sed \u0027s/^.*\\(\\/201.\\/\\)/\\1/g\u0027 | grep csv \u003e gkg2eng_allfilenames_01.txt\n$HADOOP_BIN/hdfs dfs -rm $DATASTORE_DIR/gkg2eng_allfilenames_01.txt\n$HADOOP_BIN/hdfs dfs -put ./gkg2eng_allfilenames_01.txt $DATASTORE_DIR/gkg2eng_allfilenames_01.txt\n\n# this file can now be read into spark to drive extracting the timeseries\n\n$HADOOP_BIN/hdfs dfs -cat  $DATASTORE_DIR/gkg2eng_allfilenames_01.txt | head -5\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:27 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {
          "HADOOP_HOME": ""
        },
        "forms": {}
      },
      "jobName": "paragraph_1464873183866_582044287",
      "id": "20160528-183845_1730515169",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Deleted /users/uktpmhallett/feeds/gdelt/datastore/gkg2eng_allfilenames_01.txt\n/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220160000.gkg.csv.bz2\n/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220161500.gkg.csv.bz2\n/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220163000.gkg.csv.bz2\n/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220164500.gkg.csv.bz2\n/users/uktpmhallett/feeds/gdelt/gkg2eng/20150320160000.gkg.csv.bz2\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:27 PM",
      "dateFinished": "Jun 3, 2016 5:20:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Set out the pipeline processes",
      "text": "\n//\n// This section defines the extraction process from raw GKG\n// The output is the GCAM measures alongside the first location mentioned in the Story\n//\n\n\ndef ExtractTimeSeries ( fileName: String \u003d \"/users/uktpmhallett/feeds/gdelt/gkg2eng/2016/20161010*\") : org.apache.spark.sql.DataFrame  \u003d {\n\n// the filename points to the gkgfiles(s) to extract the timeseries from\n// the GcamPrefix is the catagory of timeseries to extract. c18 \u003d the GDELT core measures\n\nval textFileName \u003d fileName.toString\n\n\nval GkgCoreSchema \u003d StructType(Array(\n    StructField(\"GkgRecordId\"           , StringType, true), //$1       \n    StructField(\"V21Date\"               , StringType, true), //$2       \n    StructField(\"V2SrcCollectionId\"     , StringType, true), //$3       \n    StructField(\"V2SrcCmnName\"          , StringType, true), //$4\n    StructField(\"V2DocId\"               , StringType, true), //$5\n    StructField(\"V1Counts\"              , StringType, true), //block $6\n    StructField(\"V21Counts\"             , StringType, true), //block $7\n    StructField(\"V1Themes\"              , StringType, true), //block $8\n    StructField(\"V2Themes\"              , StringType, true), //block $9\n    StructField(\"V1Locations\"           , StringType, true), //block $10\n    StructField(\"V2Locations\"           , StringType, true), //block $11\n    StructField(\"V1Persons\"             , StringType, true), //block $12\n    StructField(\"V2Persons\"             , StringType, true), //block $13    \n    StructField(\"V1Orgs\"                , StringType, true), //block $14\n    StructField(\"V2Orgs\"                , StringType, true), //block $15\n    StructField(\"V15Tone\"               , StringType, true), //$16\n    StructField(\"V21Dates\"              , StringType, true), //$17\n    StructField(\"V2GCAM\"                , StringType, true), //$18\n    StructField(\"V21ShareImg\"           , StringType, true), //$19\n    StructField(\"V21RelImg\"             , StringType, true), //$20\n    StructField(\"V21SocImage\"           , StringType, true), //$21\n    StructField(\"V21SocVideo\"           , StringType, true), //$22\n    StructField(\"V21Quotations\"         , StringType, true), //$23\n    StructField(\"V21AllNames\"           , StringType, true), //$24\n    StructField(\"V21Amounts\"            , StringType, true), //$25\n    StructField(\"V21TransInfo\"          , StringType, true), //$26\n    StructField(\"V2ExtrasXML\"           , StringType, true)  //$27\n    ))\n\n\n\n//val GkgInputFilePath \u003d \"/user/feeds/gdelt/gkg2eng\"\n\n//val GkgInputFilePath \u003d \"/user/feeds/gdelt/gkg2eng/2015/201512*.gkg.csv\"\n\nval GkgFileRaw \u003d sqlContext.read\n                           .format(\"com.databricks.spark.csv\")\n                           .option(\"header\", \"false\")                               // Use first line of all files as header\n                           .schema(GkgCoreSchema)\n                           .option(\"delimiter\", \"\\t\")                               // tab delimited input\n                           .load(textFileName)\n\nGkgFileRaw.registerTempTable(\"GkgFile\")\n//GkgFileRaw.show(10)\n\nval GcamRaw \u003d GkgFileRaw.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"V2GCAM\", \"V1Locations\")\n    GcamRaw.cache()\n    GcamRaw.registerTempTable(\"GcamRaw\")\n\ndef vgeoWrap (lat: Double, long: Double, len: Int): String \u003d {\n    var ret \u003d GeoHash.encodeHash(lat, long, len)\n    // select the length of the geohash, less than 12..\n    // it pulls in the library dependency from \n    return(ret)\n}\n\nsqlContext.udf.register(\"vGeoHash\", vgeoWrap(_:Double,_:Double,_:Int))\n\nval ExtractGcam \u003d sqlContext.sql(\"\"\"\n    select\n        GkgRecordID \n    ,   V21Date\n    ,   split(V2GCAM, \",\")                   as Array\n   ,    explode(split(V1Locations, \";\"))              as LocArray -- I am simplifying the location to just the very first mention.\n   -- ,   regexp_replace(V1Locations, \";.*$\", \"\") as LocArray -- this acheives it by truncating off unneeded locations after the first delimiter\n    ,   regexp_replace(V15Tone, \",.*$\", \"\") as V15Tone     -- I trunc off the other scores\n    from GcamRaw \n    where length(V2GCAM) \u003e1 and length(V1Locations) \u003e1\n \n\"\"\")\nval explodeGcamDF \u003d ExtractGcam.explode(\"Array\", \"GcamRow\"){c: Seq[String] \u003d\u003e c }\n//.explode(\"PersonArray\", \"PersonName\"){d: Seq[String] \u003d\u003e d}\n\nval GcamRows \u003d explodeGcamDF.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"GcamRow\", \"LocArray\") //,\"PersonName\")\n    GcamRows.registerTempTable(\"GcamRows\")\n\nval TimeSeries \u003d sqlContext.sql(\"\"\"\nselect \n  d.V21Date\n, d.LocCountryCode\n--, d.LocName\n, d.Lat\n, d.Long\n, vGeoHash(d.Lat, d.Long, 12) as GeoHash\n, \u0027E\u0027 as NewsLang\n, Series\n, coalesce(sum(d.Value),0) as SumValue -- in SQL \"coalesce\" means \"replaces nulls with\" ;-)\n, count(distinct  GkgRecordID ) as ArticleCount\n, Avg(V15Tone) as AvgTone\nfrom\n(   select\n    GkgRecordID\n    , V21Date\n    , ts_array[0] as Series\n    , ts_array[1] as Value\n    , loc_array[0] as LocType\n--    , loc_array[1] as LocName\n    , loc_array[2] as LocCountryCode\n    , loc_array[4] as Lat\n    , loc_array[5] as Long\n    , V15Tone\n    from\n       (select\n         GkgRecordID\n       , V21Date\n       , split(GcamRow,   \":\") as ts_array\n       , split(LocArray, \"#\") as loc_array\n       , V15Tone\n       from GcamRows\n       where length(GcamRow)\u003e1\n       ) x\n    where\n    (loc_array[0] \u003d 3 or  loc_array[0] \u003d 4)\n   -- I am removing constraints over the metrics to collect, as it breaks the translated files. \n   -- and\n   --   (           x.ts_array[0] like \u0027c18%\u0027 -- c18 denotes the gdelt specific timeseries, which are word dictionary counts.   c18.85\u003dstate of emergency\n   --      or      x.ts_array[0] \u003d \u0027wc\u0027)\n) d\ngroup by \n  d.V21Date\n, d.LocCountryCode\n--    , LocName\n, d.Lat\n, d.Long\n, vGeoHash(d.Lat, d.Long, 12)\n, d.Series\norder by \n  d.V21Date\n, vGeoHash(d.Lat, d.Long, 12)\n, d.Series\n\"\"\")\n\n   return TimeSeries\n}\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:41 PM",
      "config": {
        "lineNumbers": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_431715062",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ExtractTimeSeries: (fileName: String)org.apache.spark.sql.DataFrame\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:41 PM",
      "dateFinished": "Jun 3, 2016 5:20:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Set Parquet Parameters",
      "text": "sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\nsqlContext.setConf(\"spark.sql.parquet.filterPushdown\", \"true\")\n\n// not sure how to set this from the zeppelin application.\n//sqlContext.hadoopConfiguration.set(\"parquet.enable.summary-metadata\", \"false\")    /// doesn\u0027t work\n//hadoopConfiguration.set(\"parquet.enable.summary-metadata\", \"false\")               /// doesn\u0027t work either\n\n//sqlContext.hadoopConfiguration.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n//sqlContext.hadoopConfiguration.set(\"parquet.enable.summary-metadata\", \"false\")\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:47 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160531-211951_2043400719",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:47 PM",
      "dateFinished": "Jun 3, 2016 5:20:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load Filenames list to process sequentially",
      "text": "// Here I loop through the files to process. \n// I will write outputs to parquet as I go. It is not a restartable process.\n\nval fileNamePath \u003d \"/users/uktpmhallett/feeds/gdelt/datastore/gkg2eng_allfilenames_*.txt\"\nval files \u003d sqlContext.read\n                      .format(\"com.databricks.spark.csv\")\n                      .option(\"header\", \"true\")                               // Use first line  as header found in 00_txt\n                      .option(\"delimiter\", \"\\t\")\n                      .load(fileNamePath)\n\nfiles.registerTempTable(\"files\")\n//This should return the filenames as an array of strings\nval filearray \u003d files.select(\"filename\").limit(3)  // only take three files. \nfilearray.show(5, false )\n\n\n\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:50 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-184546_875236767",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "fileNamePath: String \u003d /users/uktpmhallett/feeds/gdelt/datastore/gkg2eng_allfilenames_*.txt\nfiles: org.apache.spark.sql.DataFrame \u003d [filename: string]\nfilearray: org.apache.spark.sql.DataFrame \u003d [filename: string]\n+------------------------------------------------------------------+\n|filename                                                          |\n+------------------------------------------------------------------+\n|/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220160000.gkg.csv.bz2|\n|/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220161500.gkg.csv.bz2|\n|/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220163000.gkg.csv.bz2|\n+------------------------------------------------------------------+\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:20:50 PM",
      "dateFinished": "Jun 3, 2016 5:20:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// loop through to avoid machine resource issues\n\nval filesToLoopThrough \u003d filearray.rdd.map(f \u003d\u003e f(0).asInstanceOf[String]).collect()  // turn the column data into an array of strings\n\n//before the loop we set the save mode to overwrite, later after the first write, we set it to append...\nval outputfile \u003d \"/users/uktpmhallet/feeds/gdelt/datastore/GcamGeo\"  // set out the file filepath by adding prefix to filename\n\n// savemode determines append to parquet or overwrite\nvar svm \u003d 0\nfor ( fname \u003c- filesToLoopThrough ){\n     var filetoprocess \u003d fname\n     var newData \u003d ExtractTimeSeries(filetoprocess)\n     // now we are ready to write the parquet file\n    if (svm \u003e 0){\n        newData.save(outputfile, \"parquet\", SaveMode.Overwrite)\n    } else {\n        newData.save(outputfile, \"parquet\", SaveMode.Append)  // TimeSeries.save(TimeSeriesParqueFile, \"parquet\", SaveMode.Append)\n        svm \u003d 1\n    }\n} // end of for loop",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:20:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464965060982_457937781",
      "id": "20160603-154420_2040235446",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "filesToLoopThrough: Array[String] \u003d Array(/users/uktpmhallett/feeds/gdelt/gkg2eng/20150220160000.gkg.csv.bz2, /users/uktpmhallett/feeds/gdelt/gkg2eng/20150220161500.gkg.csv.bz2, /users/uktpmhallett/feeds/gdelt/gkg2eng/20150220163000.gkg.csv.bz2)\noutputfile: String \u003d /users/uktpmhallet/feeds/gdelt/datastore/GcamGeo\nsvm: Int \u003d 0\nwarning: there were 2 deprecation warning(s); re-run with -deprecation for details\n"
      },
      "dateCreated": "Jun 3, 2016 3:44:20 PM",
      "dateStarted": "Jun 3, 2016 5:20:56 PM",
      "dateFinished": "Jun 3, 2016 5:25:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Now we have a proper datastore of the GCAM data in a parquet file, set up an object to retrieve data from it.\n\nval GcamParquet \u003d sqlContext.read.parquet(outputfile)\nGcamParquet.registerTempTable(\"GcamTimeSeries\")\n\nGcamParquet.printSchema",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:26:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160531-221859_1219243553",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "GcamParquet: org.apache.spark.sql.DataFrame \u003d [V21Date: string, LocCountryCode: string, Lat: string, Long: string, GeoHash: string, NewsLang: string, Series: string, SumValue: double, ArticleCount: bigint, AvgTone: double]\nroot\n |-- V21Date: string (nullable \u003d true)\n |-- LocCountryCode: string (nullable \u003d true)\n |-- Lat: string (nullable \u003d true)\n |-- Long: string (nullable \u003d true)\n |-- GeoHash: string (nullable \u003d true)\n |-- NewsLang: string (nullable \u003d true)\n |-- Series: string (nullable \u003d true)\n |-- SumValue: double (nullable \u003d true)\n |-- ArticleCount: long (nullable \u003d true)\n |-- AvgTone: double (nullable \u003d true)\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:26:00 PM",
      "dateFinished": "Jun 3, 2016 5:26:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// now run a test query over the Parquet file\n\nval TimeSeries \u003d sqlContext.sql(\"\"\"\nselect * from GcamTimeSeries\nwhere Series like \u0027c18.%\u0027 and ArticleCount \u003e 5 and LocCountryCode \u003d \"CA\"\n\"\"\")\n\nTimeSeries.show(200, false)\n\n// geohash.org\n// http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:26:06 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160529-211347_738607243",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "TimeSeries: org.apache.spark.sql.DataFrame \u003d [V21Date: string, LocCountryCode: string, Lat: string, Long: string, GeoHash: string, NewsLang: string, Series: string, SumValue: double, ArticleCount: bigint, AvgTone: double]\n+--------------+--------------+-------+--------+------------+--------+-------+--------+------------+---------------------+\n|V21Date       |LocCountryCode|Lat    |Long    |GeoHash     |NewsLang|Series |SumValue|ArticleCount|AvgTone              |\n+--------------+--------------+-------+--------+------------+--------+-------+--------+------------+---------------------+\n|20150220163000|CA            |49.25  |-123.133|c2b2nj9q11kt|E       |c18.193|51.0    |6           |-4.132600767314087   |\n|20150220163000|CA            |49.25  |-123.133|c2b2nj9q11kt|E       |c18.342|71.0    |7           |-3.7368574891497155  |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.139|18.0    |6           |0.2168427341542185   |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.14 |12.0    |8           |-0.7492723347872116  |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.147|10.0    |7           |1.0254826735935254   |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.180|33.0    |11          |-8.985815818432548E-4|\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.193|185.0   |15          |-0.7182990693320044  |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.298|16.0    |10          |-0.24011490289300258 |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.342|63.0    |12          |-2.3687031573064483  |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.35 |36.0    |6           |2.281883291362735    |\n|20150220163000|CA            |43.6667|-79.4167|dpz82v68byb7|E       |c18.71 |18.0    |7           |-2.875178273462261   |\n|20150220163000|CA            |45.4167|-75.7   |f244m7ugqnsb|E       |c18.180|33.0    |8           |-1.2828652042334088  |\n|20150220163000|CA            |45.4167|-75.7   |f244m7ugqnsb|E       |c18.193|111.0   |10          |-2.9141678810496763  |\n|20150220163000|CA            |45.4167|-75.7   |f244m7ugqnsb|E       |c18.342|53.0    |8           |-1.6461560604820118  |\n|20150220163000|CA            |45.4167|-75.7   |f244m7ugqnsb|E       |c18.35 |34.0    |6           |0.3379663263508732   |\n|20150220163000|CA            |45.5   |-73.5833|f25dve428yfb|E       |c18.193|37.0    |6           |-3.39415360031772    |\n|20150220163000|CA            |47.5   |-72     |f2ky1x55w3ub|E       |c18.180|20.0    |6           |-1.129775110881122   |\n|20150220163000|CA            |47.5   |-72     |f2ky1x55w3ub|E       |c18.193|61.0    |6           |-1.129775110881122   |\n|20150220163000|CA            |47.5   |-72     |f2ky1x55w3ub|E       |c18.342|30.0    |6           |-1.129775110881122   |\n+--------------+--------------+-------+--------+------------+--------+-------+--------+------------+---------------------+\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:26:06 PM",
      "dateFinished": "Jun 3, 2016 5:26:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "--//val TimeSeriesPivot \u003d TimeSeries.groupBy(\"V21Date\",\"LocCountryCode\").pivot(\"Series\").agg(sum(\"SumValue\") )\n--//TimeSeriesPivot.registerTempTable(\"TimeSeriesPivot\")\n--//TimeSeriesPivot.cache()",
      "dateUpdated": "Jun 2, 2016 2:13:03 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "V21Date",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "V21Date",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_870598205",
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val TimeSeriesCheck \u003d sqlContext.sql(\"\"\"\nSELECT\n    yyyymmdd\n,   sum(SumValue) as SumSeriesValue\n,   LocCountryCode\n,   regexp_replace(Series, \u0027\\\\.\u0027, \u0027\u0027) as Series\n,   Avg(AvgTone) as avgTone\n,   Sum(ArticleCount) as ArticleCount\nfrom\n(   select \n        cast(substr(V21Date,1,8) as INT) as yyyymmdd\n    ,    LocCountryCode\n    ,   Series\n    ,   SumValue\n    ,   AvgTone\n    ,   ArticleCount\n    from     GcamTimeSeries\n    where (Series \u003d \u0027wc\u0027 OR Series like \u0027c18.12%\u0027) AND (LocCountryCode \u003d \u0027FR\u0027  OR LocCountryCode \u003d \u0027UK\u0027 OR LocCountryCode \u003d \u0027GM\u0027 )  -- FRANCE/UK + REFUGEES\n) chk\ngroup by yyyymmdd, LocCountryCode, Series\norder by yyyymmdd\n\"\"\")\nTimeSeriesCheck.show(10, true)\n\n\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:26:16 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "V21Date",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "LocCountryCode",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [
            {
              "name": "Series",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "scatter": {
            "xAxis": {
              "name": "V21Date",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "LocCountryCode",
              "index": 1.0,
              "aggr": "sum"
            }
          },
          "forceY": true,
          "lineWithFocus": true
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_213001454",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "TimeSeriesCheck: org.apache.spark.sql.DataFrame \u003d [yyyymmdd: int, SumSeriesValue: double, LocCountryCode: string, Series: string, avgTone: double, ArticleCount: bigint]\n+--------+--------------+--------------+------+--------------------+------------+\n|yyyymmdd|SumSeriesValue|LocCountryCode|Series|             avgTone|ArticleCount|\n+--------+--------------+--------------+------+--------------------+------------+\n|20150220|           1.0|            GM|c18122|    1.19225037257824|           1|\n|20150220|           7.0|            UK| c1812|  -0.658217008638441|           4|\n|20150220|           4.0|            UK|c18123|   -3.45489443378119|           2|\n|20150220|          50.0|            GM|c18123|  -5.121930490584711|          16|\n|20150220|      125056.0|            UK|    wc|-0.31652447985757676|         224|\n|20150220|       20504.0|            GM|    wc|  0.8641262279124242|          54|\n|20150220|       49317.0|            FR|    wc|  0.5821120235840421|          41|\n+--------+--------------+--------------+------+--------------------+------------+\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:26:16 PM",
      "dateFinished": "Jun 3, 2016 5:26:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Need Spark 1.6 to use pivot\nval TimeSeriesPivot \u003d TimeSeriesCheck.groupBy(\"yyyymmdd\",\"LocCountryCode\").pivot(\"Series\").agg(sum(\"SumSeriesValue\")) \nTimeSeriesPivot.registerTempTable(\"TimeSeriesPivot\")\nTimeSeriesPivot.cache()\nTimeSeriesPivot.show(10)",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:37:50 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_262734425",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "TimeSeriesPivot: org.apache.spark.sql.DataFrame \u003d [yyyymmdd: int, LocCountryCode: string, c1812: double, c18122: double, c18123: double, wc: double]\nres41: TimeSeriesPivot.type \u003d [yyyymmdd: int, LocCountryCode: string, c1812: double, c18122: double, c18123: double, wc: double]\n+--------+--------------+-----+------+------+--------+\n|yyyymmdd|LocCountryCode|c1812|c18122|c18123|      wc|\n+--------+--------------+-----+------+------+--------+\n|20150220|            UK|  7.0|  null|   4.0|125056.0|\n|20150220|            GM| null|   1.0|  50.0| 20504.0|\n|20150220|            FR| null|  null|  null| 49317.0|\n+--------+--------------+-----+------+------+--------+\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:37:50 PM",
      "dateFinished": "Jun 3, 2016 5:37:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val TimeSeriesPivot2 \u003d TimeSeriesPivot.groupBy(\"yyyymmdd\").pivot( \"LocCountryCode\").agg(sum(\"c18122\")/sum(\"wc\"))\nTimeSeriesPivot2.registerTempTable(\"TimeSeriesPivot2\")\nTimeSeriesPivot2.show()",
      "authenticationInfo": {},
      "dateUpdated": "Jun 3, 2016 5:39:28 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_733404770",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "TimeSeriesPivot2: org.apache.spark.sql.DataFrame \u003d [yyyymmdd: int, FR: double, GM: double, UK: double]\n+--------+----+--------------------+----+\n|yyyymmdd|  FR|                  GM|  UK|\n+--------+----+--------------------+----+\n|20150220|null|4.877097151775263...|null|\n+--------+----+--------------------+----+\n\n"
      },
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "dateStarted": "Jun 3, 2016 5:39:28 PM",
      "dateFinished": "Jun 3, 2016 5:39:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jun 2, 2016 2:13:03 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1464873183867_581659538",
      "id": "20160528-174757_988979335",
      "dateCreated": "Jun 2, 2016 2:13:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Matts GCAM",
  "id": "2BMZPX7V4",
  "angularObjects": {
    "2BNJYEYBP:shared_process": [],
    "2BMB1FQ58:shared_process": [],
    "2BKYJ7Y9H:shared_process": [],
    "2BN1P6NEG:shared_process": [],
    "2BNHFUJ7E:shared_process": [],
    "2BNRU5MH6:shared_process": [],
    "2BN91D7FQ:shared_process": [],
    "2BKS7PNCG:shared_process": [],
    "2BMYVF2BS:shared_process": [],
    "2BKWJJAUV:shared_process": [],
    "2BP2YT469:shared_process": [],
    "2BKGK21TZ:shared_process": [],
    "2BN39285K:shared_process": [],
    "2BNB2NVRZ:shared_process": [],
    "2BNPJY11V:shared_process": [],
    "2BNNDD3D2:shared_process": [],
    "2BNUSJMYY:shared_process": [],
    "2BN82M7VH:shared_process": [],
    "2BN8Z4YKB:shared_process": [],
    "2BNE396K1:shared_process": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}